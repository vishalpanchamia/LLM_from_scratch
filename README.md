Important Links
from 02
https://poloclub.github.io/cnn-explainer/

# **Lecture 3: Pretraining LLMs vs Finetuning LLMs**

ghh

[https://arxiv.org/pdf/2005.14165](https://arxiv.org/pdf/2005.14165)

[https://openai.com/index/language-unsupervised/](https://openai.com/index/language-unsupervised/)

[https://www.harvey.ai/](https://www.harvey.ai/)

[https://www.business-standard.com/world-news/jpmorgan-chase-unveils-ai-powered-llm-suite-may-replace-research-analysts-124072600460\_1.html](https://www.business-standard.com/world-news/jpmorgan-chase-unveils-ai-powered-llm-suite-may-replace-research-analysts-124072600460_1.html)

[https://openai.com/index/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program/](https://openai.com/index/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program/)

GPT-3 paper link:   
OpenAI blog post link shown in video:  
Harvey AI Legal Assisstant:   
JP Morgan news article:   
OpenAI fine-tuning blog post: 

# **Lecture 8: The GPT Tokenizer: Byte Pair Encoding**

[https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)

# **Lecture 9: Creating Input-Target data pairs using Python DataLoader**

Datasets and DataLoaders in Python:  
[https://docs.pytorch.org/tutorials/beginner/basics/data\_tutorial.html](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)  
[https://docs.pytorch.org/docs/stable/data.html](https://docs.pytorch.org/docs/stable/data.html)

# **Lecture 11: The importance of Positional Embeddings**

[https://docs.pytorch.org/tutorials/beginner/basics/data\_tutorial.html](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)  
[https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html)

# **Lecture 12: The entire Data Preprocessing Pipeline of Large Language Models (LLMs)**

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01\_main-chapter-code/the-verdict.txt](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/the-verdict.txt)

[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)

[https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)

[https://docs.pytorch.org/tutorials/beginner/basics/data\_tutorial.html](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)

[https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html)

# **Lecture 13: Introduction to the Attention Mechanism in Large Language Models (LLMs)**

 [https://arxiv.org/pdf/1409.0473](https://arxiv.org/pdf/1409.0473)

[https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)

[https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

# **Lecture 14: Simplified Attention Mechanism \- Coded from scratch in Python | No trainable weights**

[https://docs.pytorch.org/docs/stable/generated/torch.nn.Softmax.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Softmax.html)

# **Lecture 15: Coding the self attention mechanism with key, query and value matrices**

[https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html)

# **Lecture 16: Causal Self Attention Mechanism | Coded from scratch in Python**

[https://docs.pytorch.org/docs/stable/generated/torch.triu.html](https://docs.pytorch.org/docs/stable/generated/torch.triu.html)

[https://docs.pytorch.org/docs/stable/generated/torch.tril.html](https://docs.pytorch.org/docs/stable/generated/torch.tril.html)

[https://docs.pytorch.org/docs/stable/generated/torch.Tensor.masked\_fill\_.html\#torch.Tensor.masked\_fill\_](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_)

[https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html)

[https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html)

[https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723](https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723)

# **Evaluating LLM performance on real dataset | Hands on project | Book data**

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01\_main-chapter-code/the-verdict.txt](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/the-verdict.txt)

[https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)

[https://docs.pytorch.org/tutorials/beginner/basics/data\_tutorial.html](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)

[https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross\_entropy.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html)

# **Coding the entire LLM Pre-training Loop**

[https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html](https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html)

[https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross\_entropy.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html)

[https://docs.pytorch.org/tutorials/beginner/basics/data\_tutorial.html](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01\_main-chapter-code/the-verdict.txt](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/the-verdict.txt)

# **Temperature Scaling in Large Language Models (LLMs)**

[https://docs.pytorch.org/docs/stable/generated/torch.multinomial.html](https://docs.pytorch.org/docs/stable/generated/torch.multinomial.html)

# **Top-k sampling in Large Language Models**

[https://docs.pytorch.org/docs/stable/generated/torch.topk.html](https://docs.pytorch.org/docs/stable/generated/torch.topk.html)

[https://docs.pytorch.org/docs/stable/generated/torch.multinomial.html](https://docs.pytorch.org/docs/stable/generated/torch.multinomial.html)

[https://medium.com/@harshit158/softmax-temperature-5492e4007f71](https://medium.com/@harshit158/softmax-temperature-5492e4007f71)

# **Saving and loading LLM model weights using PyTorch**

[https://docs.pytorch.org/docs/stable/generated/torch.optim.Optimizer.state\_dict.html](https://docs.pytorch.org/docs/stable/generated/torch.optim.Optimizer.state_dict.html)

[https://docs.pytorch.org/tutorials/beginner/saving\_loading\_models.html](https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html)

[https://docs.pytorch.org/tutorials/recipes/recipes/what\_is\_state\_dict.html\#what-is-a-state-dict-in-pytorch](https://docs.pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html#what-is-a-state-dict-in-pytorch)

# **Loading pre-trained weights from OpenAI GPT-**

[https://www.kaggle.com/datasets/xhlulu/openai-gpt2-weights](https://www.kaggle.com/datasets/xhlulu/openai-gpt2-weights)

# **Introduction to LLM Finetuning | Python Coding with hands-on-example**

[https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to\_csv.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)

# **Dataloaders in LLM Classification Finetuning | Python Coding | Hands on LLM project**

[https://archive.ics.uci.edu/dataset/228/sms+spam+collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection)

[https://docs.pytorch.org/tutorials/beginner/basics/data\_tutorial.html](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)

[https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)

# **Coding the model architecture for LLM classification fine-tuning**

[https://archive.ics.uci.edu/dataset/228/sms+spam+collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection)

# **Coding a fine-tuned LLM spam classification model | From Scratch**

[https://archive.ics.uci.edu/dataset/228/sms+spam+collection](https://archive.ics.uci.edu/dataset/228/sms+spam+collection)

[https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross\_entropy.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html)

[https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html](https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html)

[https://docs.pytorch.org/docs/stable/generated/torch.save.html](https://docs.pytorch.org/docs/stable/generated/torch.save.html)

# **Introduction to LLM Instruction Fine-tuning | Loading Dataset | Alpaca Prompt format**

[https://github.com/tatsu-lab/stanford\_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/instruction-data.json](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json)

# **Data Batching in LLM instruction fine-tuning | Hands on project | Live Python coding**

[https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)

[https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)

[https://github.com/tatsu-lab/stanford\_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/instruction-data.json](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json)

# **Dataloaders in Instruction Fine-tuning**

[https://docs.pytorch.org/tutorials/beginner/basics/data\_tutorial.html](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)

[https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)

[https://github.com/tatsu-lab/stanford\_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/instruction-data.json](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json)

# **Instruction fine-tuning: Loading pre-trained LLM weights**

[https://docs.pytorch.org/tutorials/beginner/basics/data\_tutorial.html](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)

[https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)

[https://github.com/tatsu-lab/stanford\_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/instruction-data.json](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json)

# **Instruction fine-tuning: Loading pre-trained LLM weights**

[https://docs.pytorch.org/tutorials/beginner/basics/data\_tutorial.html](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)

[https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)

[https://github.com/tatsu-lab/stanford\_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/instruction-data.json](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json)

# **LLM fine-tuning training loop | Coded from scratch**

[https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)

[https://github.com/tatsu-lab/stanford\_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/instruction-data.json](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json)

# **Evaluating fine-tuned LLM using Ollama**

[https://github.com/tatsu-lab/stanford\_alpaca](https://github.com/tatsu-lab/stanford_alpaca)

[https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)

[https://tatsu-lab.github.io/alpaca\_eval/](https://tatsu-lab.github.io/alpaca_eval/)

[https://arxiv.org/pdf/2009.03300](https://arxiv.org/pdf/2009.03300)

[https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01\_main-chapter-code/instruction-data.json](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json)
